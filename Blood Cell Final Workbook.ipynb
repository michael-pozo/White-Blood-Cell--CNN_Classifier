{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82246118",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4179f3",
   "metadata": {},
   "source": [
    "This notebook details my process for using a convolutional neural network (CNN) to classify white bloods by their subtypes. The dataset comes from Kaggle (https://www.kaggle.com/datasets/paultimothymooney/blood-cells) and includes approximately 12000 augmented JPEG images of cells from a microscpe. The images came prepackaged in train and test sets that were equally balanced across the four classes. Images were loaded into the keras preprocessing package ImageDataGenerator to produce the image pixel arrays and labels. As part of my EDA, I examined the distribution of RGB pixel values for each class and ran an ANOVA test to confirm they come from different populations. Inspection of the distributions revealed slight differences such as the higher density of high pixel values for Monocytes, higher density of lower pixel values for Lymphocytes and the realtively similar distributions of Eosinophils and Neutrophils. My hypothesis was that the model would pick up on these differences, making classes 1 and 2 easier to classify versus 0 and 3. The modeling process involved trialing different model complexities, batch sizes, image sizes, larning rates and regularization techniques. Results were evaluated using accuracy scores and categorical cross entropy loss. Throughout the process, test/validation scores steadily increased, but were significantly outpaced by training scores, showing signs of overfitting. Regularization techniques such as adding Dropouts and L2 regularization were trialed to varying degrees of success. Ultimately, the best model recorded an accuracy of 81% and loss of 0.545. My hypothesis around which classes would be easiest and most difficult to classify was confirmed by the confusion matrix and classification report. As next steps, I would like to continue to add more images to the training set and explore additional regularization techiniques to reduce overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4dacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from tensorflow.keras.layers import Flatten \n",
    "from tensorflow.keras.layers import Conv2D \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8f44b",
   "metadata": {},
   "source": [
    "## Load Data with ImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88edbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign train and test os paths  \n",
    "\n",
    "train_path = '/Users/mike/Flatiron/Blood_Cells/Data/dataset2-master/dataset2-master/images/TRAIN'\n",
    "test_path = '/Users/mike/Flatiron/Blood_Cells/Data/dataset2-master/dataset2-master/images/TEST'\n",
    "\n",
    "#generate image arrays and labels for train, validation and test\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split = .2)\n",
    "train_generator = train_datagen.flow_from_directory(train_path, target_size =(256,256), batch_size = 64,\n",
    "                                                   class_mode = 'categorical', subset ='training')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_path, target_size =(256,256), batch_size = 64,\n",
    "                                                  class_mode= 'categorical', shuffle = False)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(train_path, target_size =(256,256), batch_size = 64,\n",
    "                                                  class_mode= 'categorical', subset = 'validation', shuffle = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50325d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm class balance for train and test\n",
    "\n",
    "train_labels = train_generator.classes\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "train_label, train_count = np.unique(train_labels, return_counts=True)\n",
    "test_label, test_count = np.unique(test_labels, return_counts=True)\n",
    "\n",
    "print('Train ~ {}'.format(list(zip(train_label, train_count))))\n",
    "print('Test ~ {}'.format(list(zip(test_label, test_count))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b01a6e",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf561d",
   "metadata": {},
   "source": [
    "For EDA, I will visualize the distribution of pixel intensities for each class to see if there are any noticeable differences. An ANOVA test will be used to confirm the classes come from different populations and have statistically significant differences in mean pixel intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty arrays to store pixel values and labels\n",
    "pixels = []\n",
    "labels = []\n",
    "\n",
    "# iterate over batches generated by train_generator. Append values to respective lists\n",
    "for x_batch, y_batch in train_generator:\n",
    "    pixels.append(x_batch)\n",
    "    labels.append(y_batch)\n",
    "    # stop iteration when all batches have been processed\n",
    "    if len(labels) * train_generator.batch_size >= train_generator.n:\n",
    "        break\n",
    "\n",
    "# combine pixel values and labels into single arrays\n",
    "pixels = np.concatenate(pixels, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "# group pixel values by class label\n",
    "class_pixels = {}\n",
    "for i in range(train_generator.num_classes):\n",
    "    class_pixels[i] = pixels[labels[:, i] == 1]\n",
    "\n",
    "# calculate mean and standard deviation for each class\n",
    "class_stats = {}\n",
    "for i in range(train_generator.num_classes):\n",
    "    class_stats[i] = {}\n",
    "    class_stats[i]['mean'] = np.mean(class_pixels[i], axis=0)\n",
    "    class_stats[i]['std'] = np.std(class_pixels[i], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test distributions for independence\n",
    "\n",
    "alpha = .05\n",
    "#initialize empty arrays to store p-values\n",
    "p_values = []\n",
    "\n",
    "#extract pixel values for each class\n",
    "class_0_pixels = class_stats[0]['mean']\n",
    "class_1_pixels = class_stats[1]['mean']\n",
    "class_2_pixels = class_stats[2]['mean']\n",
    "class_3_pixels = class_stats[3]['mean']\n",
    "\n",
    "# perform ANOVA test on the four classes\n",
    "f_statistic, p_value = f_oneway(class_0_pixels, class_1_pixels, class_2_pixels, class_3_pixels)\n",
    "\n",
    "# append p-value to list\n",
    "p_values.append(p_value)\n",
    "\n",
    "# combine p-values using Fisher's method\n",
    "fisher_p_value = np.prod(p_values)\n",
    "np.set_printoptions(precision=10)\n",
    "\n",
    "print(\"Overall p-value:\", fisher_p_value)\n",
    "if fisher_p_value < alpha:\n",
    "    print('We reject the null hypothesis that the classes come from the same population')\n",
    "else:\n",
    "    print('We fail to reject the null hypothesis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fdb55",
   "metadata": {},
   "source": [
    "I will use seaborn to graph the distributions of the 4 classes on one plot, separated using hue. In order to do so, I will combine all the pixel values and labels into a dataframe and take a random sample of 5 million pixels from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate pixel values for each class into one array\n",
    "x = np.concatenate([class_pixels[i].ravel() for i in range(train_generator.num_classes)])\n",
    "\n",
    "# create a DataFrame with the concatenated pixel values and a holder value for class\n",
    "data = pd.DataFrame({'Pixel Values': x, 'Class':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e07ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the index range for each class \n",
    "for i in range(train_generator.num_classes):\n",
    "    print('There are {} pixels in class {}'.format(+ len(class_pixels[i].ravel()), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6db330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the number of rows in new column Class for each label\n",
    "x = 490930176\n",
    "y = 488177664\n",
    "z = 487194624\n",
    "t = 491323392\n",
    "\n",
    "# Set the values of the 'Class' column\n",
    "data.loc[:x-1, 'Class'] = 0\n",
    "data.loc[x:x+y-1, 'Class'] = 1\n",
    "data.loc[x+y:x+y+z-1, 'Class'] = 2\n",
    "data.loc[x+y+z:x+y+z+t-1, 'Class'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ed75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with a random sample of 5,000,000 rows from each class\n",
    "\n",
    "#set variables and initialize a list to hold sampled rows\n",
    "sample_size = 5000000\n",
    "class_col = 'Class'\n",
    "sampled_dataframes = []\n",
    "\n",
    "# Sample from each class and concatenate the results\n",
    "for class_val in range(train_generator.num_classes):\n",
    "    class_subset = data[data[class_col] == class_val]\n",
    "    class_sample = class_subset.sample(n=sample_size)\n",
    "    sampled_dataframes.append(class_sample)\n",
    "\n",
    "# Concatenate the sampled dataframes\n",
    "sampled_df = pd.concat(sampled_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot histogram of pixel values with each class separated by hue\n",
    "sns.histplot(sampled_df, x='Pixel Values', hue='Class', alpha=0.5, kde=True)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.4, 1.0)\n",
    "plt.ylim(0,120000)\n",
    "plt.title('Pixel Value Frequency by Class')\n",
    "plt.legend(['Eosinophil', 'Lymphocyte', 'Monocyte', 'Neutrophil'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972d50c",
   "metadata": {},
   "source": [
    "Inspection of the distribution shows a couple of interesting trends. First, Lymphocytes and Monocytes seem to have slightly wider distributions with more pixel values pushed out towards the tails in opposite directions. This tendency toward lighter and darker pixel intesities respectively may make them easier for the model to detect. The second aspect of distribution that catches my attention is that Eosinophil and Neutrophil are both more densly packed around the mean. This suggests to me that they are more similar to each other and will be more difficult to distinguish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebcf58",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df51bb",
   "metadata": {},
   "source": [
    "For the baseline model, I will create a very simple convolutionary neural network (CNN) with only one convolutional layer, max pooling, and one dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1542f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a model\n",
    "base_model = Sequential()\n",
    "\n",
    "\n",
    "# add the input layer  \n",
    "base_model.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(256, 256, 3)))\n",
    "\n",
    "\n",
    "# max pool in 2x2 window\n",
    "base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(64, activation='relu'))\n",
    "base_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "base_model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8884ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model with the training data. Start with 20 epochs at 20 steps per epoch  \n",
    "history_base = base_model.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96427962",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = base_model.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9417e7",
   "metadata": {},
   "source": [
    "Baseline test accuracy is 45% and loss is 0.4796. Training accuracy is already very high, showing that the model is overfitting off the bat. I will try to add more complexity to increase my accuracy before I attempt to fix the overfitting with regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe9eee",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de19d6b",
   "metadata": {},
   "source": [
    "In the first model, I will add more convolutional and dense layers to add complexity to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a model\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "# add the input layer \n",
    "model_1.add(Conv2D(filters=64,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(256, 256, 3)))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#add second convolutional layer\n",
    "model_1.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#add third convolutional layer\n",
    "model_1.add(Conv2D(filters=16,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(128, activation='relu'))\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "model_1.add(Dense(32, activation='relu'))\n",
    "model_1.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_1.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model with the training data.  \n",
    "history_1 = model_1.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a17bb",
   "metadata": {},
   "source": [
    "Based on the validation scores, it is clear the added complexity did not help the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fefdc",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab05a5a",
   "metadata": {},
   "source": [
    "In the second mode, I will reduce the amount of added complexity and try to use a smaller filter layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861074c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a model\n",
    "model_2 = Sequential()\n",
    "\n",
    "\n",
    "# add the input layer  \n",
    "model_2.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(256, 256, 3)))\n",
    "\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_2.add(Conv2D(filters=16,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(256, 256, 3)))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0d672",
   "metadata": {},
   "source": [
    "There is a slight improvement over Model 1, but the validation scores remain fairly consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2d919",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f68c5",
   "metadata": {},
   "source": [
    "My hypothesis is that since only one cell in the image is dyed, I can focus in on the target cell more by using a smaller image size. I will also try to use smaller batchsizes, a more narrow input layer and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regenerate the image data using 128x128 as the image size  \n",
    "\n",
    "train_path = '/Users/mike/Flatiron/Blood_Cells/Data/dataset2-master/dataset2-master/images/TRAIN'\n",
    "test_path = '/Users/mike/Flatiron/Blood_Cells/Data/dataset2-master/dataset2-master/images/TEST'\n",
    "\n",
    "#generate image arrays and labels for train, validation and test\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split = .2)\n",
    "train_generator = train_datagen.flow_from_directory(train_path, target_size =(128,128), batch_size = 32,\n",
    "                                                   class_mode = 'categorical', subset ='training')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_path, target_size =(128,128), batch_size = 32,\n",
    "                                                  class_mode= 'categorical', shuffle = False)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(train_path, target_size =(128,128), batch_size = 32,\n",
    "                                                  class_mode= 'categorical', subset = 'validation', shuffle = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a model\n",
    "model_3 = Sequential()\n",
    "\n",
    "\n",
    "# add the input layer \n",
    "model_3.add(Conv2D(filters=16,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_3.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(128, activation='relu'))\n",
    "model_3.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be210865",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = model_3.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_3.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f95d3",
   "metadata": {},
   "source": [
    "Model 3 is significantly faster and performs better than any model previously run. Test accuracy was 57% with loss of 0.91."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec7efd",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f320b5",
   "metadata": {},
   "source": [
    "The changes in model 3 seem to have created improvements. In this next model, I will add more complexity, reduce the kernel_size, and add more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf842734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Sequential()\n",
    "\n",
    "\n",
    "# add the input layer \n",
    "model_4.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_4.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_4.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(256, activation='relu'))\n",
    "model_4.add(Dense(128, activation='relu'))\n",
    "model_4.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_4.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc843e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = model_4.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8683e",
   "metadata": {},
   "source": [
    "Model 4 showed the highest validation performance yet. Improvements were consistent all the way to the end, which leads me to believe I need to add more epochs to allow it to learn longer. Since the model is so fast, I don't see this as an issue right now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22e9a6",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d3788",
   "metadata": {},
   "source": [
    "Model 5 includes an additional dense layer and uses a smaller filter size compared to the previous model. I will run this model for 100 epochs to allow more time for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential()\n",
    "\n",
    "\n",
    "# add the input layer  \n",
    "model_5.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_5.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_5.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(256, activation='relu'))\n",
    "model_5.add(Dense(128, activation='relu'))\n",
    "model_5.add(Dense(64, activation = 'relu'))\n",
    "model_5.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_5.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d848297",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5 = model_5.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_5.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06096cce",
   "metadata": {},
   "source": [
    "Model 5 has the highest test accuracy yet at 73%. Test loss is 1.216. This is currently my best model. Training accuracy is up to 97%, but the model is overfitting compared to the validation and test sets. In my next model iteration, I will try to add some regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52b1ac",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c16d23",
   "metadata": {},
   "source": [
    "In this model, Dropout commands have been added to the second and third convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d78eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = Sequential()\n",
    "\n",
    "# add the input layer \n",
    "model_6.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_6.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_6.add(Dropout(0.25))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_6.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_6.add(Dropout(0.25))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_6.add(Flatten())\n",
    "model_6.add(Dense(256, activation='relu'))\n",
    "model_6.add(Dense(128, activation='relu'))\n",
    "model_6.add(Dense(64, activation = 'relu'))\n",
    "model_6.add(Dense(32, activation = 'relu'))\n",
    "model_6.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_6.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_6 = model_6.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363629cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_6.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba55a8",
   "metadata": {},
   "source": [
    "Adding the Dropouts significantly helped with the overfitting and raised the test accuracy scores to 77%. The test loss fell by half to 0.589. I will consider Model 6 as my best model and leave the Dropout commands in subsequent iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678ebe5",
   "metadata": {},
   "source": [
    "## Model 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfb32b",
   "metadata": {},
   "source": [
    "In the following model, I would like to experiment with the learning rate. I will use a step decay approach to decrease the learning rate at later epochs to allow it to fine tune the weights after the majority of the accuracy gains have been achieved. Based on the learning progression from Model 6, this seems to have been around 75 epochs, so that will be my target inflection point. I will change the learning rate using a schedule function and the LearningRateScheduler from keras callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3538883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr, decay_factor, step_size):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7 = Sequential()\n",
    "\n",
    "# add the input layer \n",
    "model_7.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_7.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_7.add(Dropout(0.25))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_7.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_7.add(Dropout(0.25))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_7.add(Flatten())\n",
    "model_7.add(Dense(256, activation='relu'))\n",
    "model_7.add(Dense(128, activation='relu'))\n",
    "model_7.add(Dense(64, activation = 'relu'))\n",
    "model_7.add(Dense(32, activation = 'relu'))\n",
    "model_7.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_7.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "lr_sched = step_decay_schedule(1e-3, decay_factor=0.25, step_size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a25fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_7 = model_7.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 100, callbacks = [lr_sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f39893",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_7.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2d4a3",
   "metadata": {},
   "source": [
    "Model 7 returns the best results yet with 81% test accuracy and loss of 0.545."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098230c",
   "metadata": {},
   "source": [
    "## Model 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a78fb",
   "metadata": {},
   "source": [
    "In this model, I will try a different approach with the learning rate. I will drop the learning rate by 10% at two different intervals over the course of 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr, decay_factor):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        if epoch <= 75:\n",
    "            return initial_lr\n",
    "        if epoch > 75 and epoch <= 150:\n",
    "            new_lr = initial_lr - (initial_lr * decay_factor)\n",
    "            return new_lr\n",
    "        else:\n",
    "            last_lr = initial_lr - (initial_lr * (2*decay_factor))\n",
    "            return last_lr\n",
    "    \n",
    "    return LearningRateScheduler(schedule)\n",
    "\n",
    "lr_sched = step_decay_schedule(1e-3, decay_factor=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = Sequential()\n",
    "\n",
    "# add the input layer \n",
    "model_8.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_8.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Dropout(0.25))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_8.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu'))\n",
    "model_8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(Dropout(0.25))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_8.add(Flatten())\n",
    "model_8.add(Dense(256, activation='relu'))\n",
    "model_8.add(Dense(128, activation='relu'))\n",
    "model_8.add(Dense(64, activation = 'relu'))\n",
    "model_8.add(Dense(32, activation = 'relu'))\n",
    "model_8.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_8.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_8 = model_8.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 200, callbacks = [lr_sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_8.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956566bf",
   "metadata": {},
   "source": [
    "Model 8's performance is similar to model 7, but with a larger loss and is more computationally expensive. I will continue with Model 7 as my best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7d81f",
   "metadata": {},
   "source": [
    "## Model 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c26bc",
   "metadata": {},
   "source": [
    "At this point, I will shift my focus to further addressing the overfitting. I will attempt to add L2 regularization to my convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4894b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9 = Sequential()\n",
    "\n",
    "# add the input layer  \n",
    "model_9.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_9.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_9.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                  kernel_regularizer = 'l2'))\n",
    "model_9.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_9.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                   kernel_regularizer = 'l2'))\n",
    "model_9.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_9.add(Flatten())\n",
    "model_9.add(Dense(256, activation='relu'))\n",
    "model_9.add(Dense(128, activation='relu'))\n",
    "model_9.add(Dense(64, activation = 'relu'))\n",
    "model_9.add(Dense(32, activation = 'relu'))\n",
    "model_9.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_9.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "lr_sched = step_decay_schedule(1e-3, decay_factor=0.25, step_size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a12443",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_9 = model_9.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 100, callbacks = [lr_sched])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0bbdd",
   "metadata": {},
   "source": [
    "The penalty appears to be too strong and holds back the model from reaching its full predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f6731",
   "metadata": {},
   "source": [
    "## Model 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a75e9",
   "metadata": {},
   "source": [
    "The default penalty value for the L2 regularizer is 0.01. I will try to reduce that to .001 and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ff1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10 = Sequential()\n",
    "\n",
    "# add the input layer \n",
    "model_10.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "\n",
    "model_10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second convolutional layer\n",
    "model_10.add(Conv2D(filters=32,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                   kernel_regularizer = regularizers.L2(1e-3)))\n",
    "model_10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_10.add(Dropout(0.25))\n",
    "\n",
    "# add third convolutional layer\n",
    "model_10.add(Conv2D(filters=64,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                   kernel_regularizer = regularizers.L2(1e-3)))\n",
    "model_10.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_10.add(Dropout(0.25))\n",
    "\n",
    "# connect all nodes with dense layers. output for multi-categorical with 4 classes  \n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(256, activation='relu'))\n",
    "model_10.add(Dense(128, activation='relu'))\n",
    "model_10.add(Dense(64, activation = 'relu'))\n",
    "model_10.add(Dense(32, activation = 'relu'))\n",
    "model_10.add(Dense(4, activation='softmax'))\n",
    "\n",
    "#using adam optimizer, categorical_crossentropy to measure loss and accuracy as our metric  \n",
    "model_10.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56537cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_10 = model_10.fit(train_generator, steps_per_epoch = 20, verbose = 1, validation_data = val_generator,\n",
    "                              epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_10.evaluate_generator(test_generator, verbose =2)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151369d8",
   "metadata": {},
   "source": [
    "The reduced penalty did not help the overfitting at later epochs and performance remains similar to previous models. Due to time constraints, I will move forward with Model 7 as my best model and evaluate its results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66cc7b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850eca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the change in loss for the train and validation sets \n",
    "model7_history = pd.DataFrame(history_7.history)\n",
    "model7_history.index.name = 'Epochs'\n",
    "\n",
    "col_list = ['loss', 'val_loss']\n",
    "model7_history[col_list].plot()\n",
    "plt.ylabel('Categorical Cross Entropy')\n",
    "plt.title('Training Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the change in accuracy for the train and validation sets \n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "model7_history[col_list].plot()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix and classification report to see how the model performed across classes \n",
    "predictions = model_7.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "print(classification_report(true_classes, predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e314ce",
   "metadata": {},
   "source": [
    "The classification report confirms our initial hypothesis that classes 1 and 2 will be the easiest to classify due to their different pixel value distributions while classes 0 and 3 will present more of a challenge. The confusion matrix supports this by showing classes 0 and 3 are most commonly misclassified as each other. The test accuracy of 81% is a significant improvement over the baseline of 45% and warrants further exploration with the model. With more time I would like to gather more training data to improve accuracy and play with different regularization techniques/values to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152ef31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
